- [D-DETR](https://arxiv.org/pdf/2010.04159) was introduced to solve slow convergence challenges in DETR (and transformers in general).
- DETR is slow to train because the attention weights are initialized relatively uniformly but the trained weights are quite sparse. To go from non sparse to sparse takes a while during training.
- D-DETR introduces a deformable attention module based on the Deformable convolution block
	- $\text{DeformAttn}(z_q, p_q, x) = \sum_{m=1}^{M}W_m\left[\sum_{k=1}^{K}A_{mqk}\cdot W^{\prime}_m x(p_q + \Delta p_mqk)\right]$
	- $M$ is the number of attention heads. $k$ indexes the sampled keys, $K$ is the total sampled key number. $\Delta p_{mqk}$ is the sampling offset (Since this can lead to a fractional indexing bilinear interpolation is used). $A_{mqk}$  is the attention weight of the $k^{\text{th}}$ sampling point in the $m^{\text{th}}$ attention head. This scalar weight is in the range \[0, 1\] and is normalized by $\sum_{k=1}^{K} A_{mqk} = 1$.
	- $\Delta p_{mqk}$ and $A_{mqk}$ are obtained via linear projection over the query feature $z_q$. 
	- The complexity of this module is $\mathcal{O}(2N_qC^2 + \operatorname{min}(HWC^2, N_qKC^2))$, where $N_q$ is the number of query elements. 
- The next innovation of D-DETR is the use of a Multi-scale Deformable Attention module. This allows it to detect objects at various scales. Other object detection methods make use of pyramid networks to aggregate object features from multiple scales. (I believe this is what the SPPF module in YOLOv8 is). 
	- Let ${x^{l}}_{l=1}^{L}$ be multiple input feature maps at multiple scales, where $x^l \in \mathbb{R}^{C \times H_l \times W_l}$. Let $\hat{p}_q \in [0,1]^{2}$ be the normalized coordinates of the reference point for each query element q.
	- $$\text{MSDeformAttn}(z_q, \hat{p}_q, {x^l}_{l=1}^{L}) = \sum_{m=1}^{M}W_{m}\left[\sum_{l=1}^{L} \sum_{k=1}^{K} A_{mlqk} \cdot W^{\prime}_{m}x^{l}(\phi_{l}(\hat{p}_2) + \Delta p_{mlqk})\right]$$
	- In normalized coordinates (0, 0) denotes the top left of the image and (1, 1) indicates the bottom right
	- The [paper](https://arxiv.org/pdf/2010.04159) also mentions that this attention module is a generalization of the deformable convolution module. We can retrieve the deformable convolution if we choose $L=1$, $K=1$ and $W^{\prime}_m = \mathbb{I}_{C_{\nu} \times C}$
	- This multiscale deformable attention replaces the attention modules in DETR that process the feature maps. 
- The paper suggested using a CNN backbone (they use ResNet. They also call this encoder). They extract 3 feature maps (${x}_{l=1}^{L-1}$,$L=4$) from stages $C_3$, $C_4$ and $C_5$. These are then transformed using a 1x1 convolution. They extract a final feature map (to make the 4) by passing $C_5$ through a 3x3, stride 2 convolution. They call this feature map $C_6$.  Each of these feature maps have $C=256$ channels. They also say they do not use the FPN (Feature Pyramid Network) as the multiscale deformable attention exchanges information between the multiple scales. They also mention no improvement by adding an FPN in their ablation studies. 
- In application of the multi-scale deformable attention module in encoder, the output are of multi-scale feature maps with the same resolutions as the input. Both the key and query elements are of pixels from the multi-scale feature maps. For each query pixel, the reference point is itself. To identify which feature level each query pixel lies in, we add a scale-level embedding, denoted as $e_{l}$, to the feature representation, in addition to the positional embedding. Different from the positional embedding with fixed encodings, the scale-level embedding ${e_{l}}_{l=1}^{L}$ are randomly initialized and jointly trained with the network.
- Their decoder starts with a reference point and then predicts the bounding box as relative offsets from this reference point. They say this reduces optimization difficulty but provide nothing to back this claim up. They make use of the sigmoid and inverse sigmoid to predict the bounding box in normalized coordinates:
	- $\hat{b}_q = \{\sigma(b_{qx} + \sigma^{-1}(\hat{p}_{qx})), \sigma(b_{qy} + \sigma^{-1}(\hat{p}_{qy})), \sigma(b_{qw}), \sigma(b_{qh})\}$
	- This seems to predict the centre of the box as well as the width and height, all in normalized coordinates.
- In section 4.2 they suggest improvements that can be made to D-DETR.
- In Appendix 5 they compute the gradient norms for each item in the final prediction (x, y, w, h) to see what D-DETR is paying attention to. They found that the model looks at the extremities of the object to determine the size of the bounding box. They also found that for classification of the object in the bounding box, the model tends to look at a lot of pixels inside the box. This kinda makes sense when considering how humans determine the size and type of objects. We first look at the extremities to determine the size and then consider everything inside the extremities to determine what the object is.